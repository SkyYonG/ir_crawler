{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'rp_crawler', using template directory 'c:\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Code\\크롤링 프로젝트\\rp_crawler\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd rp_crawler\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!rm -rf rp_crawler\n",
    "!scrapy startproject rp_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. \"items.py\" 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rp_crawler/rp_crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rp_crawler/rp_crawler/items.py\n",
    "import scrapy\n",
    "\n",
    "class CrawlerItem(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    ir_link = scrapy.Field()\n",
    "    fs_link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. \"spiders.py\" 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rp_crawler/rp_crawler/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rp_crawler/rp_crawler/spiders/spider.py\n",
    "import scrapy\n",
    "import datetime\n",
    "\n",
    "from rp_crawler.items import CrawlerItem\n",
    "from datetime import timedelta\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name = \"RPCrawler\"\n",
    "    allow_domain = [\"https://m.irgo.co.kr/\"]\n",
    "    start_urls = [\"https://m.irgo.co.kr/IR%EC%9E%90%EB%A3%8C\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        date = response.xpath('//*[@id=\"irDataList\"]/div/div[2]/span[3]/text()').extract()\n",
    "        urls = response.xpath('//*[@id=\"irDataList\"]/div/@data-href').extract()\n",
    "        links = []\n",
    "        for i in range(len(date)):\n",
    "            if date[i][-3:] == \"1일전\":\n",
    "                links.append(urls[i])\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.page_content)\n",
    "            \n",
    "    def page_content(self, response):\n",
    "        item = CrawlerItem()\n",
    "        item[\"name\"] = response.xpath('//*[@id=\"content\"]/div[1]/div/dl/dd[1]/a/span[2]/text()')[0].extract()\n",
    "        item[\"date\"] = str(datetime.datetime.now()-timedelta(days=1))[:10]\n",
    "        try:\n",
    "            item[\"ir_link\"] = response.xpath('//*[@id=\"content\"]/div[1]/div/dl/dd[3]/a/@href')[0].extract()\n",
    "        except:\n",
    "            item[\"ir_link\"] = response.url\n",
    "        number = response.xpath('//*[@id=\"content\"]/div[1]/div/dl/dd[1]/a/@href')[0].extract()\n",
    "        code= \"A\" + str(number[29:35])\n",
    "        item[\"fs_link\"] = \"http://comp.fnguide.com/SVO2/ASP/SVD_Finance.asp?pGB=1&gicode={}\".format(code)\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \"mongodb.py\" 작성\n",
    "- \"##.###.###.###\" 대신 본인 서버 주소를 입력하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing rp_crawler/rp_crawler/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rp_crawler/rp_crawler/mongodb.py\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://##.###.###.###:27017/')\n",
    "db = client.ir_report\n",
    "collection = db.ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"pipelines.py\" 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rp_crawler/rp_crawler/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rp_crawler/rp_crawler/pipelines.py\n",
    "import datetime\n",
    "\n",
    "from scrapy.exporters import CsvItemExporter\n",
    "from datetime import timedelta\n",
    "from .mongodb import collection\n",
    "\n",
    "# csv 파일로 저장하기 위한 class\n",
    "class CsvPipeline(object):\n",
    "    def __init__(self):\n",
    "        self.file = open(\"save_csv/ir_{}.csv\".format(str(datetime.datetime.now()-timedelta(days=1))[:10]), \"wb\")\n",
    "        self.exporter = CsvItemExporter(self.file, encoding='utf-8')\n",
    "        self.exporter.start_exporting()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.exporter.export_item(item)\n",
    "        return item\n",
    "\n",
    "# mongodb에 저장하기 위한 class\n",
    "class MongdbPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        data = { \"name\": item[\"name\"], \n",
    "                 \"date\": item[\"date\"],\n",
    "                 \"ir_link\": item[\"ir_link\"], \n",
    "                 \"fs_link\": item[\"fs_link\"],\n",
    "               }\n",
    "        \n",
    "        collection.insert(data)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. \"run.sh\" 작성 (크롤러 실행 파일)\n",
    "- \"run.sh\"는 이 주피터 노트북이 있는 폴더에 저장됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "cd rp_crawler\n",
    "scrapy crawl RPCrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. \"run.sh\" 파일에 실행 권한 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x run.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
